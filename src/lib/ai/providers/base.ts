import type {
  AIProvider,
  AIModel,
  ProviderConfig,
  ChatParams,
  ChatResponse,
  ProviderMessage,
} from "../types";
import { ProxyAgent, fetch as undiciFetch } from "undici";

/**
 * è·å–ä»£ç†é…ç½®çš„ fetch å‡½æ•°
 * å¦‚æœè®¾ç½®äº† HTTPS_PROXY æˆ– HTTP_PROXY ç¯å¢ƒå˜é‡ï¼Œåˆ™ä½¿ç”¨ä»£ç†
 */
function getProxyAgent(): ProxyAgent | undefined {
  const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
  if (proxyUrl) {
    return new ProxyAgent(proxyUrl);
  }
  return undefined;
}

/**
 * ä½¿ç”¨ä»£ç†å‘é€è¯·æ±‚
 */
async function fetchWithProxy(
  url: string,
  init?: RequestInit
): Promise<Response> {
  const proxyAgent = getProxyAgent();
  
  if (proxyAgent) {
    // ä½¿ç”¨ undici çš„ fetch å’Œä»£ç†
    const response = await undiciFetch(url, {
      ...init,
      dispatcher: proxyAgent,
    } as Parameters<typeof undiciFetch>[1]);
    
    // è½¬æ¢ä¸ºæ ‡å‡† Response
    return response as unknown as Response;
  }
  
  // æ— ä»£ç†æ—¶ä½¿ç”¨åŸç”Ÿ fetch
  return fetch(url, init);
}

/**
 * OpenAI å…¼å®¹ API çš„åŸºç±»
 * SiliconFlow å’Œ Gemini éƒ½æ”¯æŒ OpenAI Chat Completion API æ ¼å¼
 */
export abstract class OpenAICompatibleProvider implements AIProvider {
  abstract id: string;
  abstract name: string;
  abstract defaultBaseUrl: string;

  /**
   * è·å–å®é™…ä½¿ç”¨çš„ Base URL
   */
  protected getBaseUrl(baseUrl?: string): string {
    return baseUrl || this.defaultBaseUrl;
  }

  /**
   * æ„å»ºè¯·æ±‚å¤´
   */
  protected buildHeaders(apiKey: string): Record<string, string> {
    return {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    };
  }

  /**
   * éªŒè¯ API Key
   * é€šè¿‡è°ƒç”¨ models æ¥å£æ¥éªŒè¯
   */
  async validateKey(apiKey: string, baseUrl?: string): Promise<boolean> {
    try {
      const models = await this.listModels(apiKey, baseUrl);
      return models.length > 0;
    } catch {
      return false;
    }
  }

  /**
   * è·å–æ¨¡å‹åˆ—è¡¨
   * å­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•ä»¥æä¾›è‡ªå®šä¹‰å®ç°
   */
  async listModels(apiKey: string, baseUrl?: string): Promise<AIModel[]> {
    const url = `${this.getBaseUrl(baseUrl)}/models`;

    const response = await fetchWithProxy(url, {
      method: "GET",
      headers: this.buildHeaders(apiKey),
    });

    if (!response.ok) {
      throw new Error(`Failed to list models: ${response.statusText}`);
    }

    const data = await response.json();

    // OpenAI æ ¼å¼çš„å“åº”
    if (data.data && Array.isArray(data.data)) {
      return data.data.map((model: { id: string; owned_by?: string }) => ({
        id: model.id,
        name: model.id,
        description: model.owned_by ? `by ${model.owned_by}` : undefined,
      }));
    }

    return [];
  }

  /**
   * æµå¼èŠå¤©
   */
  async *chat(
    config: ProviderConfig,
    params: ChatParams
  ): AsyncGenerator<string, void, unknown> {
    const url = `${this.getBaseUrl(config.baseUrl)}/chat/completions`;

    const body = this.buildRequestBody(config, params, true);

    const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
    console.log(`[${this.id}] Starting stream request to ${url}${proxyUrl ? ` (via proxy: ${proxyUrl})` : ''}`);

    const response = await fetchWithProxy(url, {
      method: "POST",
      headers: this.buildHeaders(config.apiKey),
      body: JSON.stringify(body),
    });

    if (!response.ok) {
      const error = await response.text();
      console.error(`[${this.id}] Request failed: ${response.status}`, error);
      throw new Error(`Chat request failed: ${response.status} - ${error}`);
    }

    if (!response.body) {
      throw new Error("No response body");
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = "";
    let chunkCount = 0;
    let totalContent = "";

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) {
          console.log(`[${this.id}] Stream ended normally. Total chunks: ${chunkCount}, Content length: ${totalContent.length}`);
          break;
        }

        buffer += decoder.decode(value, { stream: true });

        // å¤„ç† SSE æ ¼å¼
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";

        for (const line of lines) {
          const trimmed = line.trim();
          if (!trimmed) continue;
          
          if (trimmed === "data: [DONE]") {
            console.log(`[${this.id}] Received [DONE] signal`);
            continue;
          }

          if (trimmed.startsWith("data: ")) {
            try {
              const json = JSON.parse(trimmed.slice(6));
              const content = json.choices?.[0]?.delta?.content;
              const finishReason = json.choices?.[0]?.finish_reason;
              
              // è®°å½• finish_reasonï¼ˆå¯èƒ½æ˜¯ stop, length, content_filter, safety ç­‰ï¼‰
              if (finishReason) {
                console.log(`[${this.id}] âš ï¸ FINISH_REASON: ${finishReason} (content_filter/safety=å®¡æ ¸æ‹¦æˆª, stop=æ­£å¸¸ç»“æŸ, length=è¶…é•¿åº¦)`);
                // å¦‚æœæ˜¯å†…å®¹è¿‡æ»¤ï¼Œè®°å½•å®Œæ•´çš„å“åº”ä»¥ä¾¿è°ƒè¯•
                if (finishReason === 'content_filter' || finishReason === 'safety' || finishReason === 'SAFETY') {
                  console.warn(`[${this.id}] ğŸš« å†…å®¹è¢«å®¡æ ¸æ‹¦æˆª! å®Œæ•´å“åº”:`, JSON.stringify(json, null, 2));
                }
              }
              
              // æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
              if (json.error) {
                console.error(`[${this.id}] âŒ API Error in stream:`, JSON.stringify(json.error));
              }
              
              if (content) {
                chunkCount++;
                totalContent += content;
                yield content;
              }
            } catch (e) {
              // è®°å½•è§£æé”™è¯¯ï¼Œå¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯
              console.warn(`[${this.id}] Failed to parse SSE data:`, trimmed.slice(0, 200), e);
            }
          } else {
            // éæ ‡å‡†æ ¼å¼çš„è¡Œï¼Œå¯èƒ½æ˜¯é”™è¯¯ä¿¡æ¯
            console.log(`[${this.id}] Non-data line:`, trimmed.slice(0, 200));
          }
        }
      }
      
      // å¤„ç† buffer ä¸­å‰©ä½™çš„å†…å®¹
      if (buffer.trim()) {
        console.log(`[${this.id}] Remaining buffer:`, buffer.slice(0, 200));
      }
    } catch (error) {
      console.error(`[${this.id}] Stream error:`, error);
      throw error;
    } finally {
      reader.releaseLock();
    }
  }

  /**
   * éæµå¼èŠå¤©
   */
  async chatSync(
    config: ProviderConfig,
    params: ChatParams
  ): Promise<ChatResponse> {
    const url = `${this.getBaseUrl(config.baseUrl)}/chat/completions`;

    const body = this.buildRequestBody(config, params, false);

    const response = await fetchWithProxy(url, {
      method: "POST",
      headers: this.buildHeaders(config.apiKey),
      body: JSON.stringify(body),
    });

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`Chat request failed: ${response.status} - ${error}`);
    }

    const data = await response.json();

    return {
      content: data.choices?.[0]?.message?.content || "",
      finishReason: data.choices?.[0]?.finish_reason,
      usage: data.usage
        ? {
            promptTokens: data.usage.prompt_tokens,
            completionTokens: data.usage.completion_tokens,
            totalTokens: data.usage.total_tokens,
          }
        : undefined,
    };
  }

  /**
   * æ„å»ºè¯·æ±‚ä½“
   */
  protected buildRequestBody(
    config: ProviderConfig,
    params: ChatParams,
    stream: boolean
  ): Record<string, unknown> {
    return {
      model: params.model || config.model,
      messages: this.formatMessages(params.messages),
      temperature: params.temperature ?? 1,
      max_tokens: params.maxTokens || 4096, // é»˜è®¤ 4K tokens
      stream,
    };
  }

  /**
   * æ ¼å¼åŒ–æ¶ˆæ¯
   * å­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•ä»¥æä¾›è‡ªå®šä¹‰æ ¼å¼
   */
  protected formatMessages(
    messages: ProviderMessage[]
  ): Array<{ role: string; content: string }> {
    return messages.map((msg) => ({
      role: msg.role,
      content: msg.content,
    }));
  }
}
